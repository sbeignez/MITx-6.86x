{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 27. Lecture 14. Clustening (2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27.1. Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Understand the limitations of the K-Means algorithm (27.2.) \n",
    "* Understand how K-Medoids algorithm is different from the K-Means algorithm\n",
    "* Understand the computational complexity of the K-Means and the K-Medoids algorithms\n",
    "* Understand the importance of choosing the right number of clusters\n",
    "* Understand elements that can be supervised in unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27.2. Limitations of the K Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder:\n",
    "Based on: $Cost(C_1,..C_k, z_1, ..., z_k)$\n",
    "$$\\sum_{j=1}^k \\sum_{i \\in C_j} \\| x - z \\|^2 $$\n",
    "\n",
    "(1) Randomly initialize  \n",
    "(2) Iterate until no change in cost  \n",
    ".  (2a) group the points together with the closest center  \n",
    ".  (2b) re-calculate the centers  \n",
    "\n",
    "Limitation:\n",
    "\n",
    "#### Limitation 1:\n",
    "z's are actually not guaranteed to be the members of the original set of points x.  \n",
    "It's an issue for visualization  (#TODO why?)    \n",
    "It is not always guaranteed that the $k$ representatives $z_1,...,z_k$ are in the set of points $\\{x_1,...,x_n\\}$ (it's rarely the case)\n",
    "\n",
    "#### Limitation 2:\n",
    "The algorithm can't work with other distances, because the optimization which calculate the derivative only work with Euclidien distance (only?).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "![kmeans](img/A-schematic-diagram-of-convolution-and-max-pooling-layer-In-the-convolution-layer-a.svg)\n",
    "\n",
    "* Left plot: No generalization, resulting in a non-intuitive cluster boundary. $\\sigma_1 = \\sigma_2$\n",
    "* Center plot: Allow different cluster widths, resulting in more intuitive clusters of different sizes.\n",
    "* Right plot: Besides different cluster widths, allow different widths per dimension, resulting in elliptical instead of spherical clusters, improving the result.\n",
    "\n",
    "https://developers.google.com/machine-learning/clustering/algorithm/advantages-disadvantages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27.3. Introduction to the K-Medoids Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Medoids algorithm is a variation of the K-Means algorithm that addresses some of the K-Means algorithm's limitations.\n",
    "\n",
    "#### 27.3.a The K-Medoids Algo\n",
    "\n",
    "(1) INITIALIZATION: Randomly select the $\\{ z_1,..., z_k\\}$ **in $\\{ x_1,..., x_n \\}$**\n",
    "\n",
    "(2) LOOP: Iterate (until..)\n",
    "\n",
    "(2.1) ASIGN: Given , assign each $x_i$ to the closest $z_j$, so that the cost function is minimized\n",
    "\n",
    "$$ Cost(..) = \\sum_i min_{j=1,...,k} dist(x_i, z_j) $$ \n",
    " \n",
    "(2.2) Given $C_j$ find the best representative $z_j$ in $C_j$ such that:\n",
    "\n",
    "$$ \\sum_{x \\in Cj} dist()$$ \n",
    "is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "* The Medoid algo differs:\n",
    "  * step (1), initialisation is limited to x's (vs. totally random)\n",
    "  * step (2.1), same.\n",
    "  * step (2.2) find the cost-minizing representative for ANY given distance measure (vs. Euclidean only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27.4. Computational Complexity of K-Means and K-Medoids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the complexity:\n",
    "\n",
    "Be:\n",
    "* number of clusters: k\n",
    "* number of points: n\n",
    "* vector dimentionality: d\n",
    "\n",
    "Step | K-Mean | K-Medoids\n",
    "---- | ------ | ---------\n",
    "(1) init | $O(k)$ | $O(k)$\n",
    "(2) ..  | |\n",
    "(2.1) n points x k centers x d | $O(nkd)$ | $O(nkd)$ \n",
    "(2.2) k centers, i points = n x d | $O(nkd)$ | $O(n^2kd)$\n",
    "Total | $O(nkd)$ | $O(n^2kd)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27.5. Determining the Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.a. Choosing K\n",
    "$ 1 < K < N $\n",
    "\n",
    "1: All points are in 1 cluster (cost is the highest).  \n",
    "N: All points are its own representative (cost = 0).\n",
    "\n",
    "Case:  \n",
    "Supervised learning with very few points with labels and many points un-labelled.\n",
    "Can use Clustering as pre-processing.\n",
    "And better find the decision boundary. \n",
    "Cf: feature engineering ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5.b. Hyper-paramters\n",
    "Not totally unsupervised: \"Supervised Elements of Unsupervised Learning\" are the paramters that we need to provide to the machine \n",
    "\n",
    "* K, the number of clusters\n",
    "* The similarity / cost measure $dist()$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6901038fa2fcd02ac3ace02f6bb6643ab81c1e9472821c23450f8affb24b877b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('mitx': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
