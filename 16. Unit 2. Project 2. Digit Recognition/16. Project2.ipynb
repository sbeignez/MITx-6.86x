{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multinomial (Softmax) Regression and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The softmax function\n",
    "\n",
    "[Source](https://github.com/sylvaticus/MITx_6.86x/blob/34d3c53ed204a56ef0f485595b599a028a62a6ae/Unit%2002%20-%20Nonlinear%20Classification%2C%20Linear%20regression%2C%20Collaborative%20Filtering/Unit%2002%20-%20Nonlinear%20Classification%2C%20Linear%20regression%2C%20Collaborative%20Filtering.md)\n",
    "\n",
    "Summary from: https://en.wikipedia.org/wiki/Softmax_function\n",
    "\n",
    "The softmax function is a \"normalisation function\" defined as:\n",
    "\n",
    "$\\text{Softmax}(\\mathbf{Z} \\in \\mathbb{R}^{K}) \\in \\mathbb{R}^{+K} = \\frac{1}{\\sum_{j=1}^k e^{Z_j}} * e^{\\mathbf{Z}}$\n",
    "\n",
    "it maps a vector in $\\mathbb{R}^{K}$ to a new vector in $\\mathbb{R}^{+K}$ where all values are positive and sum up to 1, hence loosing one degree of freedom and with the output interpretable as probabilities\n",
    "the larger (in relative term) is the input $Z_K$, the larger will be its output probability\n",
    "note that, for each $K$, the map $Z_K \\to P(Z_K)$ is continuous\n",
    "it can be seen as a smooth approximation to the arg max function: the function whose value is which index has the maximum\n",
    "\n",
    "The softmax function can be used in multinomial logistic regression to represent the predicted probability for the j'th class given a sample vector $\\mathbf{x}$ and a weighting vector $\\mathbf{w}$:\n",
    "\n",
    "$P(y=j\\mid \\mathbf{x}) = \\frac{e^{\\mathbf{x}^\\mathsf{T}\\mathbf{w}j}}{\\sum{k=1}^K e^{\\mathbf{x}^\\mathsf{T}\\mathbf{w}_k}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "It can be parametrised by a parameter $\\tau$ referred as \"temperature\" in allusion to statical mechanics:\n",
    "\n",
    "$\\text{softmax}(\\mathbf{Z};\\tau \\in \\mathbb{R}) = \\frac{1}{\\sum_{j=1}^k e^{Z_j/\\tau}} * e^{\\mathbf{Z}/\\tau}$\n",
    "\n",
    "For high temperatures ($\\tau \\to \\infty$), all actions have nearly the same probability and the lower the temperature, the more expected rewards affect the probability.\n",
    "For a low temperature ($\\tau \\to 0^{+}$), the probability of the action with the highest expected reward tends to 1.\n",
    "\n",
    "[Source](https://cs.stackexchange.com/questions/79241/what-is-temperature-in-lstm-and-neural-networks-generally): \n",
    "\n",
    "Temperature is a hyperparameter of LSTMs (and neural networks generally) used to control the randomness of predictions by scaling the logits before applying softmax. For example, in TensorFlow’s Magenta implementation of LSTMs, temperature represents how much to divide the logits by before computing the softmax.\n",
    "\n",
    "When the temperature is 1, we compute the softmax directly on the logits (the unscaled output of earlier layers), and using a temperature of 0.6 the model computes the softmax on 𝑙𝑜𝑔𝑖𝑡𝑠0.6, resulting in a larger value. Performing softmax on larger values makes the LSTM more confident (less input is needed to activate the output layer) but also more conservative in its samples (it is less likely to sample from unlikely candidates). Using a higher temperature produces a softer probability distribution over the classes, and makes the RNN more “easily excited” by samples, resulting in more diversity and also more mistakes.\n",
    "\n",
    "Neural networks produce class probabilities with logit vector 𝐳 where 𝐳=(𝑧1,…,𝑧𝑛) by performing the softmax function to produce probability vector 𝐪=(𝑞1,…,𝑞𝑛) by comparing 𝑧𝑖 with with the other logits.\n",
    "\n",
    "𝑞𝑖=exp(𝑧𝑖/𝑇)∑𝑗exp(𝑧𝑗/𝑇)(1)\n",
    "where 𝑇 is the temperature parameter, normally set to 1.\n",
    "\n",
    "The softmax function normalizes the candidates at each iteration of the network based on their exponential values by ensuring the network outputs are all between zero and one at every timestep.\n",
    "\n",
    "Temperature therefore increases the sensitivity to low probability candidates. In LSTMs, the candidate, or sample, can be a letter, a word, or musical note, for example:\n",
    "\n",
    "* For high temperatures (𝜏→∞), all samples have nearly the same probability and the lower the temperature, the more expected rewards affect the probability.\n",
    "* For a low temperature (𝜏→0+), the probability of the sample with the highest expected reward tends to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effects of Adjusting Temperature:\n",
    "* Smaller temperature parameter means that there is less variance in our distribution, and larger temperature, more variance.\n",
    "* In other words smaller temperature parameter favors larger thetas, and larger temperature parameter makes the distribution more uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Changing Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification Using Manually Crafted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Dimensionality Reduction via PCA\n",
    "\n",
    "Principal Components Analysis (PCA) is the most popular method for linear dimension reduction of data and is widely used in data analysis.  \n",
    "\n",
    "For an in-depth exposition see: https://online.stat.psu.edu/stat505/lesson/11.\n",
    "\n",
    "Briefly, this method finds (orthogonal) directions of maximal variation in the data. By projecting an  dataset  onto  of these directions, we get a new dataset of lower dimension that reflects more variation in the original data than any other -dimensional linear projection of . By going through some linear algebra, it can be proven that these directions are equal to the  eigenvectors corresponding to the  largest eigenvalues of the covariance matrix , where  is a centered version of our original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cubic Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to compute the poly kernel?\n",
    "Which part of the course? ..\n",
    "\n",
    "https://en.wikipedia.org/wiki/Polynomial_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "In 2-D, let $ x = [x_1, x_2]$\n",
    "\n",
    "Find $\\phi(x) = [f_1(x_1,x_2), f_2(x_1,x_2), ..f_N(x_1,x_2)]$\n",
    "\n",
    "Such as:\n",
    "$$ \\phi(x) \\cdot \\phi(y) = (x^T y + 1 )^3 $$\n",
    "\n",
    "Expand: $$ (x^T y + 1 )^3 $$\n",
    "$$ = ([x, y] \\begin{bmatrix}x \\\\ y\\end{bmatrix} + 1 )^3$$\n",
    "$$ = (x^2 + y^2 + 1)^3 $$\n",
    "\n",
    "$$ = 1 + 3 x^2 + 3 x^4 + x^6 + 3 y^2 + 6 x^2 y^2 + 3 x^4 y^2 + 3 y^4 + 3 x^2 y^4 + y^6 $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$[x_1^3, \\sqrt(3)*x_1^2*x_2, \\sqrt(3)*x_1^2, \\sqrt(3)*x_1*x_2^2, \\sqrt(6)*x_1*x_2, \\sqrt(3)*x_1, x_2^3, \\sqrt(3)*x_2^2, \\sqrt(3)*x_2, 1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Kernel Methods"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
