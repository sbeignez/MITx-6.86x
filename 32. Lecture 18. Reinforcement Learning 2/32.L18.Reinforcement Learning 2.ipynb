{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 32. Lecture 18. RL 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.1 Revisiting the MDP Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MDP: $<S, A, T, R>$  \n",
    "\n",
    "T(s,a,s'): \"transition probability, the only thing that matters is the state you are in. It doesn't matter how you arrive, what you did before.\" = independent of the states visited before.  \n",
    "R(s,a,s'): same.  \n",
    "\n",
    "$\\pi^*(s)$ is the optimal policy which record the actions leading to the best total reward/utility (from next step, and the following until the end), not the best immediate reward (for the next step only).  \n",
    "  \n",
    "In the real world, $T$ and $R$ are unknown a-priori, so RL MDP algo starts with $<S, A>$ and learn them while exploring.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.2. Estimating Inputs for RL algorithm\n",
    "\n",
    "Estimating Transition Probabilities and Rewards\n",
    "\n",
    "#### Option (1): \n",
    "\n",
    "With all data:\n",
    "$$ \\hat{T} = \\frac{ count(s,a,s') }{ \\sum_{s'} count(s,a,s')}$$\n",
    "\n",
    "$ \\hat{T} = the number of actions leading to s' over the total number of actions\n",
    "\n",
    "Issues:\n",
    "* Certain states might not be visited at all while exploring\n",
    "* Certain states might be visited much less often than others leading to very noisy estimates\n",
    "\n",
    "#### Option (2) Model Free vs Model Based Approaches\n",
    "\n",
    "Model Based Approach first tries to estimates the probability distribution before estimating the expectation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 32.3. Q value iteration by sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Samples-based approach\n",
    "\n",
    "$ sample_1: R(s,a,s_1') + \\gamma max_{a'} Q(s_1',a')$  \n",
    "$ sample_i: ...$  \n",
    "$ sample_k: R(s,a,s_k') + \\gamma max_{a'} Q(s_k',a')$  \n",
    "\n",
    "Let's take average:\n",
    "\n",
    "$$ Q(s,a) = {1 \\over k} \\sum_i^k sample_i = {1 \\over k} \\sum_i^k (R(s,a,s_i') + \\gamma max_{a'} Q(s_i',a')) $$\n",
    "\n",
    "Incremental update\n",
    "\n",
    "#### Exponential running average:\n",
    "\n",
    "$$ \\hat{X}_n = \\frac{ {X}_n + (1- \\alpha){X}_{n-1} + (1- \\alpha)^2 {X}_{n-2} + ... }{a + () + ()^2 + ...}$$  \n",
    "\n",
    "In its recursive form:\n",
    "$$ \\hat{X}_n = \\alpha {X}_n + (1- \\alpha) * {X}_n$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q_{i+1}(s,a) = \\alpha * sample + (1- \\alpha) * Q_i(s,a) $$\n",
    "\n",
    "(1) Init:\n",
    "\n",
    "(2) Iterate until convergence\n",
    "\n",
    "(2.1) Collect sample: s, a, s', R(s,a,s')\n",
    "(2.2)\n",
    "$$ Q_{i+1}(s,a) = \\alpha * [ R(s,a,s') + \\gamma max_{a'} Q(s', a')] + (1- \\alpha) * Q_i(s,a) $$\n",
    "\n",
    "\n",
    "Convergence?\n",
    "$$ Q_i(s,a) + \\alpha R(s,a,a' + \\gamma * max_a Q_i() - Q..$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 34.4. Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-greedy approach\n",
    "\n",
    "$\\epsilon$-greedy approach tries to balance exploration and exploitation:  \n",
    "* by randomly sampling an action with probability: $\\epsilon$ [exploration]\n",
    "* and by choosing the best currently available option with probability: $1 - \\epsilon$ [explotation]\n",
    "\n",
    "$\\epsilon$ (thus exploration) should decrease step after step during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6901038fa2fcd02ac3ace02f6bb6643ab81c1e9472821c23450f8affb24b877b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('mitx': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
