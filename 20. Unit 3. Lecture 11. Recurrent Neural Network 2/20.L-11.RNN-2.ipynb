{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20. Lecture 11. Recurrent Neural Network (2/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Markov model to recurrent neural networks (RNNs)\n",
    "\n",
    "* Formulate, estimate and sample sequences from Markov models.\n",
    "* Understand the relation between RNNs and Markov model for generating sequences.\n",
    "* Understand the process of **decoding** of RNN in generating sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Markov models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RNN Deeper Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoding:\n",
    "* Initial state $S_{t-1}$: the vector representation-encoding of the sequence (sentence, image, sound..)\n",
    "* Initial vector $x$: the <beg> word\n",
    "* Create a probability distribution (using softmax)\n",
    "* Sample a word from that distribution\n",
    "* Create an output state $S_t$\n",
    "Repeat for next step:\n",
    "* New state: \n",
    "* New x vector: The previous word sampled (and not the probability distribution)\n",
    "* etc.. Sequentially sample a sequence\n",
    "Until:\n",
    "* Sampled word is <end>\n",
    "\n",
    "Modality:\n",
    "* can directly translate an encoded image (vector representaiton of an image) into a sentence\n",
    "* any sequence type (encoded vector) into another sequence type (prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Q1\n",
    "\n",
    "Now, which of the following is true about decoding RNN's? \n",
    "\n",
    "* [ ] In the translating example above, the output probability distribution  is fed as an input to the next step  \n",
    "* [ ] The probability distribution  is the same at each step, just like how parameters are shared between steps  \n",
    "* [ ] In the first image, the foreign word \"Olen\" in the above picture is a \"sampled\" result from a distribution the RNN produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Q2 Prediction\n",
    "\n",
    "Suppose we are building an RNN model to translate images into sentences, as described in the lecture. Which of the following is only done in generating the predictions of sentences from a trained RNN given the test images but not in the training process?\n",
    "\n",
    "* [ ] Feeding the sampled output as part of the input to the next time step  \n",
    "* [ ] Calculating what percentage of words the RNN correctly generated  \n",
    "* [ ] Feeding in the hidden state as input each time step  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6901038fa2fcd02ac3ace02f6bb6643ab81c1e9472821c23450f8affb24b877b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('mitx': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
