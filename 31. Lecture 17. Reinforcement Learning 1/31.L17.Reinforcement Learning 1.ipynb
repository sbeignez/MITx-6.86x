{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.1. Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.3. XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.4. Utility Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards:\n",
    "\n",
    "How to aggregate reward?\n",
    "Utility function\n",
    "\n",
    "Issue: infinite number of reward\n",
    "Bound the utility function\n",
    "\n",
    "We consider two different types of utility functions:\n",
    "\n",
    "Option 1: Finite horizon based utility\n",
    "\n",
    "#### Option 1: Finite horizon based utility\n",
    "\n",
    "   The utility function is the sum of rewards after acting for a fixed number $n$ steps. For example, in the case when the rewards depend only on the states, the utility function is\n",
    "\n",
    "   $$ U[s_0,...,S_n] = \\sum_{i=0}^n R(s_i) $$\n",
    " \n",
    "   In particular $U[s_0,...,S_{n+m}] = U[s_0,...,S_n]$ for any positive integer $m$. \n",
    "\n",
    "Issues: The outcome of action depends only to the state, but not to the time to arrive to this state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Option 2: (Infinite horizon) discounted reward based utility\n",
    "   In this setting, the reward one step into the future is discounted by a factor $\\gamma$, the reward two steps ahead by , and so on. The goal is to continue acting (without an end) while maximizing the expected discounted reward. The discounting allows us to focus on near term rewards, and control this focus by changing . For example, if the rewards depend only on the states, the utility function is\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   U &= R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + ... \\\\\n",
    "   &= \\sum_{t=0}^n \\gamma^t R(s_t)\n",
    "   \\end{align}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding of Utility function:\n",
    "$$\n",
    "\\begin{align}\n",
    "U &= \\sum_{t=0}^\\infin \\gamma^t R(s_t) \\\\\n",
    "&\\le R(s_{max}) \\sum_{t=0}^\\infin \\gamma^t \\\\\n",
    "&\\le {R_{max} \\over 1 - \\gamma}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $R_{max}$ is the maximal reward obtenible in any\n",
    "state,\n",
    "\n",
    "That make the algorithm converge\n",
    "\n",
    "Note:\n",
    "* For $\\gamma = 0$, $U = R(s_0) + 0 * ...$, so maximising for discounted reward is equivalent to greedily maximising for immediate reward (current step 0).\n",
    "* Discounted reward $U \\le {R_{max} \\over 1 - \\gamma}$, so if $ R_{max}$ is finite, then $U$ is also finite.\n",
    "* Discounted reward is bounded (but we don't know if it converges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31.4.a Finite Horizon vs Discounted Reward\n",
    "\n",
    "The main problem for MDPs is to optimize the agent's behavior. To do so, we first need to specify the criterion that we are trying to maximize in terms of accumulated rewards. We will define an **utility function** and maximize its expectation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.5. Policy and Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 31.5.a Policy definition\n",
    "A **policy** is a function, noted $\\pi$, $ \\pi : s \\to a $, that assigns an action $\\pi(s)$ to any state $s$.  \n",
    "An **optimal policy** is the optimal action that you can take in a state, the action that maximise the expected utility (here: the expected discounted reward).  \n",
    "The optimal policy is denoted $\\pi^*$.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6901038fa2fcd02ac3ace02f6bb6643ab81c1e9472821c23450f8affb24b877b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('mitx': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
