{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 31. Lecture 17. Reinforcement Learning 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.1. Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.2. Learning to Control: Introduction to Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning to Control: Introduction Reinforcement Learning (RL)\n",
    "\n",
    "Supervised Learning SL | Reinforcement Learning RL\n",
    "---------------------- | -------------------------\n",
    "supervision at every steps | none or limited supervision\n",
    ". | no reward at every single step\n",
    ". | what count is the final outcome, ultimate reward. <br/>And trace back what made the exploration succesful (propagate back)\n",
    "\n",
    "Plan:\n",
    "- MDP Markov Decision PRocess\n",
    "- Bellman equation (propagate back the state vaslue)\n",
    "- Value iteration algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 31.2.a. Objective of RL:  \n",
    "[ ] The goal of RL is to minimize the loss between predictions and labels for points in a labelled training dataset  \n",
    "[ ] The goal of RL is to learn a good policy with none or limited supervision  \n",
    "[ ] For an RL algorithm to work, it must a receive a non-zero reward after every step  \n",
    "[ ] RL works by maximizing the reward for each immediate next step  \n",
    "\n",
    "Posts:\n",
    "*Referring to previous video: we don't give to the machine a reward for every action, but what counts whether the whole game has been successful or not. Therefore, do we really \"maximize reward at each step\"?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.3. RL Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Markov decision process (MDP)** is defined by\n",
    "\n",
    "* 1) a set of states $s \\in S$;\n",
    "\n",
    "* 2) a set of actions $ a \\in A$;\n",
    "\n",
    "* 3) Action dependent transition probabilities $T(a,s,s') = P(s' | s,a) $, so that for each state $s$ and action $a$, $\\sum_{s' \\in S} T(a,s,s') = 1$.\n",
    "\n",
    "* 4) Reward functions $R(s,a,s')$ representing the reward for starting in state , taking action  and ending up in state  after one step. (The reward function may also depend only on $s$, or only $s$ and $a$.)\n",
    "\n",
    "MDPs satisfy the **Markov property** in that the transition probabilities and rewards depend only on the current state and action, and remain unchanged regardless of the history (i.e. past states and actions) that leads to the current state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of MDP:  \n",
    ". | . | +1\n",
    "--- | --- | ---\n",
    ". | XXX | -1\n",
    ". | . | Start\n",
    "\n",
    "\n",
    "An AI agent navigates in the 3x3 grid depicted above.\n",
    "\n",
    "* In the $3x3$ grid, the middle square is not accessible by the agent. Thus, the total number of possible state is 8.\n",
    "\n",
    "The MDP is defined as follows:\n",
    "\n",
    "* Every state  is defined by the current position of the agent in the grid (and is independent of its previous actions and positions).\n",
    "* The actions $A$ are the 4 directions “up\", “down\",“left\", “right\".\n",
    "* The transition probabilities from state  via action  to state  is given by .\n",
    "* The agent receives a reward of $+1$ for arriving at the top right cell, and a reward of $-1$ for arriving in the cell immediately below it. It does not receive any non-zero reward at the other cells as illustrated in the following figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercices\n",
    "\n",
    "#### 31.3.a. Markov Property\n",
    "\n",
    "https://ocw.mit.edu/resources/res-6-012-introduction-to-probability-spring-2018/part-i-the-fundamentals/MITRES_6_012S18_Textbook.pdf p58\n",
    "\n",
    "#### 31.3.b. Markovian Setting\n",
    "\n",
    "$ s_0, a_1 \\to s_1, a_2 \\to ... \\to s_{n-1}, a_n \\to s_n $  \n",
    "Current state $s_n$\n",
    "\n",
    "\n",
    "#### 31.3.c. Number of States\n",
    "\n",
    "#### 31.3.d. Transition Probabilities\n",
    "\n",
    "Note:\n",
    "* *Note that here, $s_i$ and $s_k$ can be any pair of states, not necessarily reachable by an action in one step.*\n",
    "* \n",
    "From posts:\n",
    "* Number of actions = 4 *(here we only have 'up', 'down', 'left', 'right')* \n",
    "* *a lot of your entries may turn out to be 0, but they still count as entries in M*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.4. Utility Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rewards:\n",
    "\n",
    "How to aggregate reward?\n",
    "Utility function\n",
    "\n",
    "Issue: infinite number of reward\n",
    "Bound the utility function\n",
    "\n",
    "We consider two different types of utility functions:\n",
    "\n",
    "Option 1: Finite horizon based utility\n",
    "\n",
    "#### Option 1: Finite horizon based utility\n",
    "\n",
    "   The utility function is the sum of rewards after acting for a fixed number $n$ steps. For example, in the case when the rewards depend only on the states, the utility function is\n",
    "\n",
    "   $$ U[s_0,...,S_n] = \\sum_{i=0}^n R(s_i) $$\n",
    " \n",
    "   In particular $U[s_0,...,S_{n+m}] = U[s_0,...,S_n]$ for any positive integer $m$. \n",
    "\n",
    "Issues: The outcome of action depends only to the state, but not to the time to arrive to this state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Option 2: (Infinite horizon) discounted reward based utility\n",
    "   In this setting, the reward one step into the future is discounted by a factor $\\gamma$, the reward two steps ahead by , and so on. The goal is to continue acting (without an end) while maximizing the expected discounted reward. The discounting allows us to focus on near term rewards, and control this focus by changing . For example, if the rewards depend only on the states, the utility function is\n",
    "\n",
    "\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   U &= R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + ... \\\\\n",
    "   &= \\sum_{t=0}^n \\gamma^t R(s_t)\n",
    "   \\end{align}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding of Utility function:\n",
    "$$\n",
    "\\begin{align}\n",
    "U &= \\sum_{t=0}^\\infin \\gamma^t R(s_t) \\\\\n",
    "&\\le R(s_{max}) \\sum_{t=0}^\\infin \\gamma^t \\\\\n",
    "&\\le {R_{max} \\over 1 - \\gamma}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $R_{max}$ is the maximal reward obtenible in any\n",
    "state,\n",
    "\n",
    "That make the algorithm converge\n",
    "\n",
    "Note:\n",
    "* For $\\gamma = 0$, $U = R(s_0) + 0 * ...$, so maximising for discounted reward is equivalent to greedily maximising for immediate reward (current step 0).\n",
    "* Discounted reward $U \\le {R_{max} \\over 1 - \\gamma}$, so if $ R_{max}$ is finite, then $U$ is also finite.\n",
    "* Discounted reward is bounded (but we don't know if it converges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[] The action at state $s$ that maximizes a finite horizon based utility can depend on how many steps have been taken.  \n",
    "[] The action at state $s$ that maximizes a finite horizon based utility does not depend on how many steps have been taken  \n",
    "[] The action at state $s$ that maximizes a discount reward based utility does not depend on how many steps have been taken.  \n",
    "[] The action at state $s$ that maximizes a discount reward based utility can depend on how many steps have been taken  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31.4.a Finite Horizon vs Discounted Reward\n",
    "\n",
    "The main problem for MDPs is to optimize the agent's behavior. To do so, we first need to specify the criterion that we are trying to maximize in terms of accumulated rewards. We will define an **utility function** and maximize its expectation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 31.5. Policy and Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 31.5.a Policy definition\n",
    "A **policy** is a function, noted $\\pi$, $ \\pi : s \\to a $, that assigns an action $\\pi(s)$ to any state $s$.  \n",
    "An **optimal policy** is the optimal action that you can take in a state, the action that maximise the expected utility (here: the expected discounted reward).  \n",
    "The optimal policy is denoted $\\pi^*$.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal policy - Numerical Example\n",
    "\n",
    "$U() = R(s_0,a_1) + \\gamma R(s_1,a_2) + \\gamma^2 R(s_2,a_3) + ...$ \n",
    "\n",
    "For $\\gamma = 0$,  \n",
    "$ U = R(s_0,a_1)$   \n",
    "$ ..  = movement reward + landing reward $   \n",
    "$ ..  = (-10) + argmax_{a}( LR(s_0, a) )$ \n",
    "\n",
    "Consider all actions, Then minimize.\n",
    "\n",
    "For $\\gamma = 0.5$,  \n",
    "$ U = 0.5 * movement rewards * number of steps + 0.5 * \\sum(LR) $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6901038fa2fcd02ac3ace02f6bb6643ab81c1e9472821c23450f8affb24b877b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('mitx': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
