
So now, given that, how can I compute
the likelihood of generating a second set of documents?
So let's say somebody gave me all this theta W's.
I have them.
How do I compute the likelihood of generating a document?

So we would assume that we have our document--
and this is the capital D, and I will use it throughout this
lecture--
given particular thetas.
So if we assume that our documents has n words.
Again, as I said earlier, every single word
is generated independently.
So we can just take a product from i from 1 to n,
and then we can just write theta W i.
So we take probability of every word in our document
and just multiply them all together.
So now, we will take this same formula
and write it in slightly different way.
And the reason we want to do it is
because in the current trita,
if we have the same word which appears
multiple times in the document, we
are going to separately multiply its probabilities.
So instead, we can actually refactorize it,
and just say we will take the probability of this word--
let's say that it appear five times--
and just make this probability in the fifth degree.
So then, instead of making a product
over all the words in the document,
we will actually go over all the words in our vocabulary.
So this is vocabulary from above.
And now we will take the probability for each word, W,
and put it in count W in our document.
So those are the two forms to write exactly the same things,
but I will primarily use this representation.
So now we are ready to start addressing the first questions
that I wrote over there, which is a question of estimation.
But before we go there, I just want
to give you an example of how this model, very simple model,
can be used to evaluate the likelihoods of the document.
So let's take the primitive example,
and I will just look at the case whenever my vocabulary just
have two words, cat and dog.
Just two words.
And now I am going to write two different models that
operate over this vocabulary.
So this would be the first model--
this is model number one--
which takes parameters theta.
And I will decide that in this particular model, theta
of cat--
the likelihood of generating the word cat--
would be 0.3.
And the likelihood of the word dog in this case--
obvious correct-- is going to be 0.7.
As you can see, they sum to 1.
They both are not negative.
And now I am going to look at model number two.
Model number two-- and to distinguish the parameter,
I just wrote theta prime.
And here, let's say we want the theta prime to generate
the word cat be 0.9.
So you can see this model really strongly prefers cats.
And then for the dog, again, because they have to sum to 1,
it's going to be 0.1.
So we have two models.
And as you can expect, they will be generating
different types of documents.
This model would love dog style of documents,
and this one is a cat model.
It will prefer cats.
So now assume that I give you a document, D,
and this particular document will have 2 cats.
It has 2 words, cat and cat, and then it has dog.
And what we can do-- we can now compute the likelihood
of these document being generated by the first model
or by the second model.
And the notations that we are going to be using here-- we're
going to say, what is the probability of this document,
D, given parameters theta.
And again, this is the values of these parameters.
So in this case, if I just straightforwardly apply
this formula, we are going to get 0.3 squared multiply
by 0.7.
And we can compute this number.
Similarly, for the second model, again, we have the document, D,
but now our parameters, theta prime, the cats,
are going to have 0.9 squared multiplied by 0.1.
And even though I didn't complete the whole computation,
it's quite easy to see that this document would
have higher probability when generated by the second model.