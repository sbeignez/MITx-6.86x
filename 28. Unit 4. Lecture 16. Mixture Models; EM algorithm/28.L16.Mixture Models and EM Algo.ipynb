{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 28. Lecture 16. Mixture Models and the Expectation Maximization (EM) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.1. Mixture Models and the Expectation Maximization (EM) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Review Maximum Likelihood Estimation (MLE) of mean and variance in Gaussian statistical model\n",
    "* Define Mixture Models\n",
    "* Understand and derive ML estimates of mean and variance of Gaussians in an Observed Gaussian Mixture Model\n",
    "* Understand Expectation Maximization (EM) algorithm to estimate mean and variance of Gaussians in an **Unobserved Gaussian Mixture Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.2. Recap of Maximum Likelihood Estimation for Multinomial and Gaussian Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "So far, in clustering we have assumed that the data has no probabilistic generative model attached to it, and we have used various iterative algorithms based on similarity measures to come up with a way to group similar data points into clusters.  \n",
    "\n",
    "(A) \n",
    "(A.1) Multinomials \n",
    "In case of language: Such as mono, bi-nomials, ..\n",
    "Given:\n",
    "- W, the vocabulary, the set of all words\n",
    "- $\\theta$. the likelihood\n",
    "  - $ P(w|\\theta) = \\theta_w $\n",
    "Assuming:\n",
    "- Sum is 1: $ \\sum_w \\theta_w = 1$\n",
    "- all theta >= 0 \n",
    "\n",
    "Then:\n",
    "* The likelihood to generate a document D, a list of words w_1,..w_.. \n",
    "  * P(D | \\theta) = \\prod \n",
    "\n",
    "(A.2) Gaussians\n",
    "\n",
    "Defined by:\n",
    "- $\\mu$, a Center\n",
    "- $\\sigma^2$, a variance\n",
    "- d is the dimension of the vector\n",
    "\n",
    "$$ P(x | ) = {1 \\over 2 \\pi \\sigma^2}$$\n",
    "\n",
    "(B) Parameter estimation? ==> MLE Max Likelihood estimator\n",
    "\n",
    "Maximising using the trick: Derivative = 0 \n",
    "\n",
    "For Guassians:\n",
    "$$ \\hat\\mu = 1/n \\sum_{i=1}^n X_i $$\n",
    "\n",
    "$$ \\hat\\sigma^2 = 1/nd * \\sum \\| x - \\mu\\|^2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Intro\n",
    "In this lecture, we will assume an underlying probabilistic generative model that will lead us to a natural clustering algorithm called the **EM algorithm**.\n",
    "  \n",
    "While a “hard\" clustering algorithm like k-means or k-medoids can only provide a cluster ID for each data point, the EM algorithm, along with the generative model driving its equations, can provide the posterior probability (“soft\" assignments) that every data point belongs to any cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:  \n",
    "* iid: Independent and identically distributed.\n",
    "* Normal distribution property: N(mu,sigma) = mu + sigma*N(0,1)\n",
    "* Find the max likelihood ==> Derive and solve = 0\n",
    "* https://stats.stackexchange.com/questions/461198/mle-under-gaussian-noise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:  \n",
    "\n",
    "* $N_i$ being iid Gaussian random variables with zero mean.\n",
    "* $N_i$ being non-iid Gaussian random variables with zero mean.\n",
    "* $N_i$ being or not being iid Gaussian random variables with or without zero mean.\n",
    "  * I.e. for the last one you have to figure out what the conditions exactly mean for the  (the 'possibly' can be figured out what it is based on that)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.3. Introduction to Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additions to previous models:\n",
    "* Mixture components: More that 1 component which has its gaussian parameters\n",
    "* Mixture weights\n",
    "\n",
    "for K clusters:\n",
    "$$ N(x, \\mu_j, \\sigma^2_j), for\\ j\\ in [1,K] $$\n",
    "\n",
    "\n",
    "Soft clustering  \n",
    "The model can calculate the probabilities that $x$ belongs to each gaussian\n",
    "\n",
    "$$ p(x|\\theta) = \\sum_j^K p_j * N(x, \\mu_1, ..., \\sigma^2_1, ...) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video 2\n",
    "\n",
    "Marginalize?\n",
    "\n",
    "$$ p(x | \\theta) = \\sum_j^k p_j * \\mathcal{N}(x, ...) = ..$$\n",
    "\n",
    "Likelihood of a set of points:\n",
    "\n",
    "$$ p( S_n | \\theta ) = \\prod \\sum p_j * \\mathcal{N}( x, ...) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.4. Mixture Model - Observed Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective:\n",
    "How to solve and find the paramters (the mixture weights $p_j$, the $\\mu$, and $\\sigma^2$) ?\n",
    "\n",
    "Simpler case: aka. the **\"observed case\"**  \n",
    "We know where each point belong\n",
    "\n",
    "Notation:  \n",
    "\n",
    "Indicator function, equal to 1 when point is assigned to cluster, 0 otherwise\n",
    "\n",
    "$ \\partial(j | i ) = 1, x_i is assigned to j \\ 0, otherwise$\n",
    "\n",
    "Solving:\n",
    "\n",
    "From: \n",
    "\n",
    "$$ p( S_n | \\theta ) = \\prod \\sum p_j * \\mathcal{N}( x, ...) $$\n",
    "\n",
    "We add log? and indicator function\n",
    "\n",
    "$$ p( S_n | \\theta ) = \\sum_{i=1}^{n\\ points} \\sum_{j=1}^{K\\ clusters} \\partial(j | i ) * log\\ p_j * \\mathcal{N}( x, ..., I??) $$\n",
    "\n",
    "We switch the 2 sums:\n",
    "\n",
    "$$ p( S_n | \\theta ) =  \\sum_{j=1}^{K\\ clusters} ( \\sum_{i=1}^{n\\ points} \\partial(j | i ) * log\\ p_j * \\mathcal{N}( x, ..., I??) ) $$\n",
    "\n",
    "Computation can be made for each cluster K and sum after.\n",
    "\n",
    "### Results:\n",
    "\n",
    "$$ \\hat p_j = \\hat n_j / n $$\n",
    "Where $\\hat n_j = \\sum_i^n \\partial(j|i)$ is the number of points in cluster $j$, and n is the total number of points.\n",
    "\n",
    "$$ \\hat \\mu_j = { 1 \\over \\hat n_j} * \\sum..$$\n",
    "\n",
    "$$ \\hat\\sigma^2_j = { 1 \\over \\hat n_j d} * \\sum ..  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* $\\mu_1$ is the mean of the points in cluster 1.\n",
    "* In $\\mu_{i,j}$, $i$ refers to the cluster and $j$ to the $j$ th coordinate.\n",
    "\n",
    "From comments:  \n",
    "* Given that all points in cluster 1 have a negative second component, so will the second component of $\\mu_{1,2}$ (TA) \n",
    "* the answers should be in scalar format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 28.5. Mixture Model - Unobserved Case: EM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-Step\n",
    "\n",
    "$ p(j|i) = {p_j * N() \\over p(X | \\theta)} $\n",
    "\n",
    "### M-Step\n",
    "So this is a step when I want to recompute the pj, the mu j,\n",
    "and the variance for cluster j.\n",
    "\n",
    "Repeat Step E/M until convergeance\n",
    "\n",
    "Guarantees related to EM algo\n",
    "* Converge locally\n",
    "* if we are starting with different random starting points, we may get very different answer, depending where we start.\n",
    "\n",
    "Intialization\n",
    "* can for ex, run K means and get mu to initialize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Notes:\n",
    "* it is possible that EM converges to a local min, a local max, or a saddle point of the likelihood function.\n",
    "* EM is not guaranteed to converge to a local minimum. It is only guaranteed to converge to a point with zero gradient with respect to the parameters. So it can indeed get stuck at saddle points.\n",
    "* The EM algorithm is deterministic, meaning that upon repeated initializations, a given set of starting values will necessarily converge to the same solution. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6901038fa2fcd02ac3ace02f6bb6643ab81c1e9472821c23450f8affb24b877b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('mitx': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
