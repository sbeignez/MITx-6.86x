{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![K1](30.2.Figure_1.png)\n",
    "![K2](30.2.Figure_2.png) \n",
    "![K3](30.2.Figure_3.png) \n",
    "![K4](30.2.Figure_4.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From comments:\n",
    "\n",
    "### Tips for Q4-b\n",
    "discussion posted by Armand_Rosta\n",
    "Look at plots for K=4 for both k-Means and EM. As you can see, the EM clusters tend to move their means to dense areas (i.e. three of the clusters are very \"overlapped\"). In contrary, the K-Means centroids tend to get apart. I hope this will help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bayesian Information Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have simply set the number of mixture components $K$ but this is also a parameter that we must estimate from data. How does the log-likelihood of the data vary as a function of $K$ assuming we avoid locally optimal solutions?\n",
    "\n",
    "To compensate, we need a selection criterion that penalizes the number of parameters used in the model. The Bayesian information criterion (BIC) is a criterion for model selection. It captures the tradeoff between the log-likelihood of the data, and the number of parameters that the model uses. The BIC of a model $M$ is defined as:\n",
    "\n",
    "$$ BIC(M) = l - \\frac{1}{2}p log n $$\n",
    "\t \n",
    "where:\n",
    "* $l$ is the log-likelihood of the data under the current model (highest log-likelihood we can achieve by adjusting the parameters in the model),\n",
    "* $p$ is the number of free parameters,\n",
    "* and $n$ is the number of data points.  \n",
    "\n",
    "This score rewards a larger log-likelihood, but penalizes the number of parameters used to train the model. In a situation where we wish to select models, we want a model with the the highest BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Mixture models for matrix completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now extend our Gaussian mixture model to predict actual movie ratings. Let $X$ again denote the  data matrix. The rows of this matrix correspond to users and columns specify movies so that X[u,i] gives the rating value of user  for movie  (if available). Both  and  are typically quite large. The ratings range from one to five stars and are mapped to integers . We will set X[u,i]=0 whenever the entry is missing.  \n",
    "\n",
    "In a realistic setting, most of the entries of  are missing. For this reason, we define  as the set of movies (column indexes) that user  has rated and  as its complement (the set of remaining unwatched/unrated movies we wish to predict ratings for). We use  to denote the number of observed rating values from user . From the point of view of our mixture model, each user  is an example X[u,:]. But since most of the coordinates of  are missing, we need to focus the model during training on just the observed portion. To this end, we use  as the vector of only observed ratings. If columns are indexed as , then a user  with a rating vector , where zeros indicate missing values, has , , and .  \n",
    "\n",
    "In this part, we will extend our mixture model in two key ways.\n",
    "\n",
    "* First, we are going to estimate a mixture model based on partially observed ratings. See notes below.\n",
    "\n",
    "* Second, since we will be dealing with a large, high-dimensional data set, we will need to be more mindful of numerical underflow issues. To this end, you should perform most of your computations in the log domain. Remember, . This can be useful to remember when and  are very small â€“ in these cases, addition should result in fewer numerical underflow issues than multiplication.\n",
    "\n",
    "An additional numerical optimization trick that you will find useful is the LogSumExp trick. Assume that we wish to evaluate . We define . Then, . This is just another trick to help ensure numerical stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Marginalizing over unobserved coordinates\n",
    "\n",
    "If  were a complete rating vector, the mixture model from Part 1 would simply say that . In the presence of missing values, we must use the marginal probability  that is over only the observed values. This marginal corresponds to integrating the mixture density  over all the unobserved coordinate values. In our case, this marginal can be computed as follows.\n",
    "\n",
    "The mixture model for a complete rating vector is written as:\n",
    "\n",
    "$$ P(x^{(u)}|\\theta) = \\sum ... $$\n",
    " \n",
    "We can decompose the multivariate spherical Gaussian as a product of univariate Gaussians (since there is no covariance between coordinates).\n",
    "\n",
    "$$ .. $$\n",
    " \t\t\t\t \t \t\t\t \t \n",
    "For , we can marginalize over all of the unobserved values to get\n",
    "\n",
    "$$ .. $$\n",
    " \n",
    "Thus, our mixture density can be written as\n",
    "\n",
    "$$ .. $$\n",
    " \n",
    "where  is the identity matrix in  dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
