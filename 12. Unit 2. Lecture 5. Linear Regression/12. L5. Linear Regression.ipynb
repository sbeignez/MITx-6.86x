{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Unit 2. Lecture 5. Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1. Unit 2 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2. Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* write the training error as **least squares** criterion for linear regression\n",
    "* use stochastic gradient descent for fitting linear regression models\n",
    "* solve closed-form linear regression solution\n",
    "* identify regularization term and how it changes the solution, generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture 1 about cassification  \n",
    "$ y^{t} in \\{-1,1\\} $\n",
    "\n",
    "Binary to continuous\n",
    "$ y^{t} in \\{-1,1\\}  ==> y^{t} in \\real $\n",
    "\n",
    "$ f(X,\\theta,\\theta_0) = \\sum_{i=1}^d \\theta \\cdot x_i + \\theta_0 $\n",
    "\n",
    "1. Objective: Which \\theta to select?\n",
    "2. Learning algo: How to learn ?\n",
    "   - Gradient based / closed form\n",
    "3. Regularization: For better generalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4. Empirical Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$ R_n (\\theta) = {1 \\over n}*\\sum (deviation)^2 / 2$  \n",
    "\n",
    "$ deviation = ( y^{(i)} - \\theta \\cdot x^{(i)})$ \n",
    "\n",
    "2 types of mistakes:\n",
    "* structural mistakes \n",
    "* estimation mistakes\n",
    "\n",
    "> Structural mistakes: The dataset is not linear\n",
    "\n",
    "> Estimation mistake: The number of data is not large enought\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5. Gradient Based Approached\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nudge the parameter in the oposite direction of the gradient.\n",
    "\n",
    "$\n",
    "\\Delta_\\theta  ( y^{(i)} - \\theta \\cdot x^{(i)})^2  \n",
    "\\\\ with \\ F(x) = xË†2; G(\\theta) = y^{(i)} - \\theta \\cdot x^{(i)}\n",
    "\\\\ = F'(G) * G'\n",
    "\\\\ = - ( y^{(i)} - \\theta \\cdot x^{(i)}) * x^{(i)}\n",
    "$\n",
    "\n",
    "Algo:\n",
    "1. Init: \\theta = 0\n",
    "2. Pick a random $t$  in $ \\{1..n\\} $\n",
    "3. Update Theta\n",
    "   - $ \\theta =  \\theta - (\\eta) * (- ( y^{(i)} - \\theta \\cdot x^{(i)}) * x^{(i)})$ \n",
    "     - With $ \\eta $ the learning rate\n",
    "   - $ \\theta =  \\theta + \\eta \\cdot ( y^{(i)} - \\theta \\cdot x^{(i)}) * x^{(i)} $\n",
    "\n",
    "   - $ \\eta_k = {1 \\over 1 + k} $; k the iteration step. In order to reduce the step size\n",
    "\n",
    "Notes:\n",
    "- correction at every step (vs. classification, correction only if mis-classified)\n",
    "- algo is self-correcting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.6. Closed form Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Not always possible. In this case, the function is convexe, so it's possible.\n",
    "\n",
    "Derivative (gradient) of Rn relative to theta\n",
    "\n",
    "$ \\delta_\\theta R_n = \\\\\n",
    "= -b + {1 \\over n} \\sum \\\\\n",
    "= -b + A \\theta = 0 \\\\\n",
    "\\\\\n",
    "\\hat{\\theta} = A^{-1}b\n",
    "$\n",
    "\n",
    "Considerations:  \n",
    "1/ Equation is solvable if A is reversible / invertible\n",
    "\n",
    "if vector Xn (x1..xn) span r^d (the rank of the matrix)\n",
    "Size of training set >> dimensionality of the feature\n",
    "\n",
    "2/ cost associated?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.7. Generalization and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization:  \n",
    "* What: Push away from theta to increase far from zero\n",
    "* Why: Avoid over-fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.8 Regularization\n",
    "\n",
    "Ridge Regression\n",
    "$ J_{\\lambda,n}(\\theta) = a.\\lambda.||\\theta||^2 + R_n(\\theta)$\n",
    "\n",
    "$ min\\ of\\ J_{()} $\n",
    "* Regularization term $\\lambda$\n",
    "  - If $\\lambda >> inf$, then $\\theta$ must be the smallest possible (and the thetas the smallest possbile) >> close to zero or generalize\n",
    "    \n",
    "  - If If $\\lambda << small$, then the regulation become small (and less important) and the loss function must be the smallest >> fit the data\n",
    "\n",
    "Loss function\n",
    "\n",
    "$$ J_{n,\\lambda} (\\theta, \\theta_0) = {1 \\over n} \\sum_{t=1}^n {(y^{(t)} - (\\theta \\cdot x^{(t)} + \\theta_0 ))^2 \\over 2}  + {\\lambda \\over 2} ||\\theta||^2 $$\n",
    "\n",
    "Algo: \n",
    "* init: $\\theta$ = 0\n",
    "* random t data point,\n",
    "  - update $\\theta = \\theta - \\eta * \\delta (J)$\n",
    "\n",
    "  - $ = (1 - \\eta\\lambda)\\theta + ...$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 12.9 Closing Comment\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
