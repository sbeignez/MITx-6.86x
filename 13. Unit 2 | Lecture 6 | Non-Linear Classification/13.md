# 13.6.1 Nonlinear Classification

## 1. Objective

At the end of this lecture, you will be able to
- derive non-linear classifiers from feature maps
- move from coordinate parameterization to weighting examples
- compute kernel functions induced from feature maps
- use kernel perceptron, kernel linear regression
- understand the properties of kernel functions

## 2. Higher Order Feature Vector

Feature map
Higher dimension
Kernel function

### a. linear classifier on the real line

#### a1. 1 dimension 


#### a.2 2 dimensions
$$
X \rarr \phi(X) = \begin{bmatrix} X \\ X^2 \end{bmatrix}, with \phi_1,\phi_2
$$

h()

## 4. Motivation for Kernel: computational efficiency

Example:
$$
x = \binom{x_1}{x_2}
\\
\phi(x) = \begin{bmatrix} x_1 \\ x_2 \\ x_1^2 \\ \sqrt{2}x_1x_2 \\x_2^2 \end{bmatrix} 
\\
\phi(x) \cdot \phi(x') = \begin{bmatrix} x_1 \\ x_2 \\ x_1^2 \\ \sqrt{2}x_1x_2 \\x_2^2 \end{bmatrix} \cdot \begin{bmatrix} x'_1 \\ x'_2 \\ {x'}_1^2 \\ \sqrt{2}x'_1x'_2 \\{x'}_2^2 \end{bmatrix}
\\
\phi(x) \cdot \phi(x') = (x \cdot x') + (x \cdot x')^2

$$

Inner product between 2 feature vectors = ..

Kernel function: $$ K(x,x') = \phi(x) \cdot \phi(x') $$


## 5. The Kernel Perceptron Algorithm

Initialisation: $\alpha$ to 0, i.e. all $\alpha_1, \alpha_2, .., \alpha_n $ to 0

Outter loop:
* for t in 1,..,T

Inner loop: (all data points in the training set)  
* for i = 1,...,n

Test:
* if
* $$ y_i * \sum_{j=1}^n \alpha_j y^{(j)} K(x^{(i)},x^{(j)})$$

Update:
* $ \alpha_i = \alpha_i + 1$

## 6. Kernel Composition Rules

Feature engineering, kernels

> A kernel is a function,  
> $ K(x,x') : \R^n Ã— \R^n \rarr \R $  
> such as:
> Exist $ \phi $ s.t $ K(x,x') = \phi(x) \cdot \phi(x') $

The kernel trick

###  Composition

1. 


2.


3. The sum of 2 kernels is a kernel

4. The product of 2 kernels is a kernel

Ex: identity kernel

## 7. The Radial Basis Kernel

$$ K(x,x') = \exp{(-1/2*||x - x'||^2)} $$


