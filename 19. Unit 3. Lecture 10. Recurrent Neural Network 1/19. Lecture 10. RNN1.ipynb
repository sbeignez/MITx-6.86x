{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19. Lecture 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Intro to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issue: Modeling sequences and temporal ?\n",
    "\n",
    "1. Using feed forward network\n",
    "   * Timeserie\n",
    "     Create a feature vector of last n time periods\n",
    "   * Language modeling\n",
    "     Create a feature vector of the last n words. Concatenated from one-hot endocing of each work in the bag of word.\n",
    "\n",
    "What are we missing?\n",
    "* how many steps to look back?\n",
    "\n",
    "\n",
    "\"While in feed-forward networks we have to manually engineer how history is mapped to a feature vector (representation) for the specific task at hand, using RNN's this task is also part of the learning process and hence automatised.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Context for Predicting Next Word\n",
    "\n",
    "What is the issue with predicting the next word in the sentence using the previous three words as context?\n",
    "(Choose all that apply.)\n",
    "\n",
    "* [ ] Some words might need more context to predict\n",
    "* [ ] Some words might need less context to predict, and additional words could be inefficient\n",
    "* [ ] Some words might be closely related to words far away in the sentence\n",
    "* [ ] Longer words are harder to predict because they have more letters\n",
    "\n",
    "a c wrong\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Why we need RNNs\n",
    "\n",
    "Predicitons tasks:\n",
    "* predict the next word in a sentence\n",
    "* sentiment classification \n",
    "* machine translation\n",
    "\n",
    "Each tasks might use different vector representation of the sentence.\n",
    "\n",
    "FOr these task, 2 steps are necessary: Encoding and decoding\n",
    "* **Encoding**: turning a sequence into a vector\n",
    "* **Decoding**: turning (that) vector into a prediction - sequence\n",
    "   * need summarization , cf Languange modeling later ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Q2\n",
    "\n",
    "U\n",
    "All of the above tasks that you selected should use the same vector representation of the sentence.\n",
    "\n",
    "\n",
    "true\n",
    "false\n",
    "correct\n",
    "In order to accomplish the tasks you selected above, which two steps are necessary?\n",
    "\n",
    "\n",
    "mapping a sequence to a vector\n",
    "mapping a vector to a prediction\n",
    "mapping a prediction to a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding with RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$ s_t = tanh( W^{s,s} s_{t-1} + W^{s,x} x_t) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gating and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Very deep \n",
    "Information is forgotten, over-written\n",
    "\n",
    "Concerns:\n",
    "* Gradients Vanish \n",
    "* Gradient Explode\n",
    "\n",
    "### A gating network\n",
    "\n",
    "$$ g_t = sigmoid( W^{g,s} s_{t-1} + W^{g,x} x_t) $$\n",
    "between [0,1]\n",
    "\n",
    "$$ s_t = (1 - g_t) * s_{t-1} + g_t * tanh(W s + W x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$S_{t-1}$ is the previous state\n",
    "$tanh()$ is the new suggested value\n",
    "\n",
    "When $g_t = 0$: learn nothing\n",
    "* $(1 - g_t) = 1$: $S_t = S_{t-1}$\n",
    "* tanh() is 0\n",
    "When g_t = 1: remember nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 the LSTM = Long Short Term Memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Calculations\n",
    "\n",
    "W (weight matrices)= [[1]]\n",
    "H (hiddent) = [1] *\n",
    "c_t (memory) = [1] * \n",
    "\n",
    "x = 5\n",
    "\n",
    "$ h_t = o_t * tanh(c_t) $\n",
    "\n",
    "$\n",
    "o_t = sigmoid( W h + W x) \\\\\n",
    "= sigmoid (1*1 + 1*5) \\\\\n",
    "= sigmoid(6) \\\\\n",
    "= { 1 \\over 1 + e^{-5}}\n",
    "= 1\n",
    "$\n",
    "\n",
    "$\n",
    "c_t = f_t * c_{t-1} + i_t * tanh()\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6901038fa2fcd02ac3ace02f6bb6643ab81c1e9472821c23450f8affb24b877b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('mitx': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
