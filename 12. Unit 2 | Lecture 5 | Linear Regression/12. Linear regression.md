# 12. Unit 2. Lecture 5. Linear Regression

## 1. Unit 2 Overview

## 2. Lecture 5 Objectives

At the end of this lecture, you will be able to
* write the training error as **least squares** criterion for linear regression
* use stochastic gradient descent for fitting linear regression models
* solve closed-form linear regression solution
* identify regularization term and how it changes the solution, generalization

## 3. Introduction

Lecture 1 about cassification  
$ y^{t} in \{-1,1\} $

Binary to continuous
$ y^{t} in \{-1,1\}  ==> y^{t} in \real $

$ f(X,\theta,\theta_0) = \sum_{i=1}^d \theta \cdot x_i + \theta_0 $

1. Objective: Which \theta to select?
2. Learning algo: How to learn ?
   - Gradient based / closed form
3. Regularization: For better generalization

## 4. Empirical Risk

$ R_n (\theta) = {1 \over n}*\sum (deviation)^2 / 2$  

$ deviation = ( y^{(i)} - \theta \cdot x^{(i)})$ 

2 types of mistakes:
* structural mistakes 
* estimation mistakes

> Structural mistakes: The dataset is not linear

> Estimation mistake: The number of data is not large enought

## 5. Gradient Based Approached

Nudge the parameter in the oposite direction of the gradient.

$
\Delta_\theta  ( y^{(i)} - \theta \cdot x^{(i)})^2  
\\ with \ F(x) = xË†2; G(\theta) = y^{(i)} - \theta \cdot x^{(i)}
\\ = F'(G) * G'
\\ = - ( y^{(i)} - \theta \cdot x^{(i)}) * x^{(i)}
$

Algo:
1. Init: \theta = 0
2. Pick a random $t$  in $ \{1..n\} $
3. Update Theta
   - $ \theta =  \theta - (\eta) * (- ( y^{(i)} - \theta \cdot x^{(i)}) * x^{(i)})$ 
     - With $ \eta $ the learning rate
   - $ \theta =  \theta + \eta \cdot ( y^{(i)} - \theta \cdot x^{(i)}) * x^{(i)} $

   - $ \eta_k = {1 \over 1 + k} $; k the iteration step. In order to reduce the step size

Notes:
- correction at every step (vs. classification, correction only if mis-classified)
- algo is self-correcting

## 6. Closed form Solution

Not always possible. In this case, the function is convexe, so it's possible.

Derivative (gradient) of Rn relative to theta

$ \delta_\theta R_n = \\
= -b + {1 \over n} \sum \\
= -b + A \theta = 0 \\
\\
\hat{\theta} = A^{-1}b
$

Considerations:  
1/ Equation is solvable if A is reversible / invertible

if vector Xn (x1..xn) span r^d (the rank of the matrix)
Size of training set >> dimensionality of the feature

2/ cost associated?


## 12.7. Generalization and Regularization

Regularization:
What: Push away from theta to increase far from zero
Why: Avoid over-fitting

## 12.8 Regularization

Ridge Regression
$ J_{\lambda,n}(\theta) = a.\lambda.||\theta||^2 + R_n(\theta)$

$ min\ of\ J_{()} $
* Regularization term $\lambda$
  - If $\lambda >> inf$, then $\theta$ must be the smallest possible (and the thetas the smallest possbile) >> close to zero or generalize
    
  - If If $\lambda << small$, then the regulation become small (and less important) and the loss function must be the smallest >> fit the data

Loss function

$$ J_{n,\lambda} (\theta, \theta_0) = {1 \over n} \sum_{t=1}^n {(y^{(t)} - \theta \cdot x^{(t)} - \theta_0 )^2 \over 2}  + {\lambda \over 2} ||\theta||^2 $$

Algo: 
* init: \theta = 0
* random t data point,
  - update \theta = \theta - \eta * \delta (J)

  - $ = (1 - \eta\lambda)\theta + ...$

## 12.9 Closing Comment

