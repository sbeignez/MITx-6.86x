# 12. Unit 2. Lecture 5. Linear Regression

## 1. Unit 2 Overview

## 2. Lecture 5 Objectives

At the end of this lecture, you will be able to
* write the training error as **least squares** criterion for linear regression
* use stochastic gradient descent for fitting linear regression models
* solve closed-form linear regression solution
* identify regularization term and how it changes the solution, generalization

## 3. Introduction

Lecture 1 about cassification  
$ y^{t} in \{-1,1\} $

Binary to continuous
$ y^{t} in \{-1,1\}  ==> y^{t} in \real $

$ f(X,\theta,\theta_0) = \sum_{i=1}^d \theta \cdot x_i + \theta_0 $

1. Objective: Which \theta to select?
2. Learning algo: How to learn ?
   - Gradient based / closed form
3. Regularization: For better generalization

## 4. Empirical Risk

$ R_n (\theta) = {1 \over n}*\sum (deviation)^2 / 2$  

$ deviation = ( y^{(i)} - \theta \cdot x^{(i)})$ 

2 types of mistakes:
* structural mistakes 
* estimation mistakes

> Structural mistakes: The dataset is not linear

> Estimation mistake: The number of data is not large enought

## 5. Gradient Based Approached

Nudge the parameter in the oposite direction of the gradient.

$
\Delta_\theta  ( y^{(i)} - \theta \cdot x^{(i)})^2  
\\ with \ F(x) = xË†2; G(\theta) = y^{(i)} - \theta \cdot x^{(i)}
\\ = F'(G) * G'
\\ = - ( y^{(i)} - \theta \cdot x^{(i)}) * x^{(i)}
$

Algo:
1. Init: \theta = 0
2. Pick a random $t$  in $ \{1..n\} $
3. Update Theta
   - $ \theta =  \theta - (\eta) * (- ( y^{(i)} - \theta \cdot x^{(i)}) * x^{(i)})$ 
     - With $ \eta $ the learning rate
   - $ \theta =  \theta + \eta \cdot ( y^{(i)} - \theta \cdot x^{(i)}) * x^{(i)} $

   - $ \eta_k = {1 \over 1 + k} $; k the iteration step. In order to reduce the step size

Notes:
- correction at every step (vs. classification, correction only if mis-classified)
- algo is self-correcting